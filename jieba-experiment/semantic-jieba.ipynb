{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import jieba, gensim, re, sqlite3, os\n",
    "import jieba.posseg as pseg\n",
    "import codecs\n",
    "from hanziconv import HanziConv\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/4_/b_9vfbwx41g4t48jrzr0r4vm0000gn/T/jieba.cache\n",
      "Loading model cost 0.465 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# dump sql to raw txt\n",
    "\n",
    "# conn = sqlite3.connect('apple.sqlite3')\n",
    "# cursor = conn.execute(\"SELECT content from apple\")\n",
    "# removetokens = [u'翻攝畫面']\n",
    "# def get_sentences(c):\n",
    "#     i = 0\n",
    "#     for w in c:\n",
    "#         context = w[0]\n",
    "#         context = re.sub(ur'[^\\u4e00-\\u9fff]+', ' ', context)\n",
    "#         context = HanziConv.toSimplified(context)\n",
    "#         for r in removetokens:\n",
    "#             context = re.sub(r, ' ', context)\n",
    "#         sentences = context.split(' ')\n",
    "#         for s in sentences:\n",
    "#             if len(s) > 10:\n",
    "#                 temp = pseg.cut(s)\n",
    "#                 words = []\n",
    "#                 semantics = []\n",
    "#                 for t in temp:\n",
    "#                     words.append(t.word)\n",
    "#                     semantics.append(t.flag)\n",
    "#                 yield '/'.join(words)+ '>>>' + '/'.join(semantics)\n",
    "\n",
    "#     c.close()\n",
    "# sentences = get_sentences(cursor)\n",
    "# with codecs.open('sentences.txt', 'w', encoding='utf8') as f:\n",
    "#     for s in sentences:\n",
    "#         f.write(s+'\\n')\n",
    "# cursor.close()\n",
    "# conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# extract semantic representation\n",
    "\n",
    "# with codecs.open('semantics.txt', 'w', encoding='utf8') as g:\n",
    "#     with codecs.open('sentences.txt', 'r', encoding='utf8') as f:\n",
    "#         for line in f.readlines():\n",
    "#             idx = line.rfind('>>>')\n",
    "#             target = line[idx + 3:]\n",
    "#             g.write(target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train word2vec model\n",
    "class MySentences(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    " \n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            with open(self.dirname + '/' + fname, 'r') as f:\n",
    "                for line in f.readlines():\n",
    "                    sen = line[:-1].split('/')\n",
    "                    yield sen\n",
    "\n",
    "# print os.listdir('testdir')\n",
    "sentences = MySentences('testdir')\n",
    "# i = 0\n",
    "# for s in sentences:\n",
    "#     print s\n",
    "#     if i > 10:\n",
    "#         break\n",
    "model = gensim.models.Word2Vec(sentences, window = 5, min_count = 5, size = 30, iter = 500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n"
     ]
    }
   ],
   "source": [
    "# debug\n",
    "print len(model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zg', 'n', 'n', 'n', 'n']\n",
      "(3, ['n', 'x', 'uj', 'ns'])\n",
      "---------------------------------------------\n",
      "['t', 'v', 'p', 'vn', 'v', 'v', 'n', 'b']\n",
      "(7, ['b', 'nrt', 'vn', 'an'])\n",
      "---------------------------------------------\n",
      "['t', 'd', 'v', 'v', 'a', 'x', 'x', 'uj', 'n']\n",
      "(2, ['v', 'mq', 'd', 'n'])\n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# make pseudo cloze quiz for validation\n",
    "import random\n",
    "\n",
    "def make_cloze(num):\n",
    "    gidx = range(0, 20178)\n",
    "    random.shuffle(gidx)\n",
    "    gidx = sorted(set(gidx[: num]))\n",
    "    \n",
    "    curr_idx = 0\n",
    "    sentences = []\n",
    "    with open('testdir/semantics.txt', 'r') as f:\n",
    "        for i in gidx:\n",
    "            for j in xrange(i - curr_idx): f.next()\n",
    "            sentences.append(f.next().split('/')[:-1])\n",
    "            curr_idx = i + 1\n",
    "     \n",
    "    options = []\n",
    "    keys = model.vocab.keys()\n",
    "    for j in xrange(num):\n",
    "        l = len(sentences[j])\n",
    "        if(l <= 0):\n",
    "            print gidx[j]\n",
    "        idx = range(0, 51)\n",
    "        random.shuffle(idx)\n",
    "        \n",
    "        candidate = [keys[i] for i in idx[:3]]\n",
    "#         count = 0\n",
    "#         while count < 10:\n",
    "        i = random.randint(0, l - 1)\n",
    "#             if len(s[i]) > 4 or count == 9:\n",
    "        options.append((i, [sentences[j][i]] + candidate))\n",
    "#                 break\n",
    "#             count += 1\n",
    "    return sentences, options\n",
    "\n",
    "sentences, options = make_cloze(3)\n",
    "for i in xrange(len(sentences)):\n",
    "    print sentences[i]\n",
    "    print options[i]\n",
    "    print '---------------------------------------------'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cloze test method\n",
    "def cloze_test(string, index, candidates):\n",
    "    temp = string[:]\n",
    "    del temp[index]\n",
    "    ref_vec = np.zeros(30)\n",
    "    l = 0\n",
    "    for w in temp:\n",
    "        if w in model.vocab:\n",
    "            ref_vec += model[w]\n",
    "        l += 1\n",
    "            \n",
    "#     ref_vec /= float(l)\n",
    "#     ref_vec = ref_vec / np.linalg.norm(ref_vec)\n",
    "    nc = len(candidates)\n",
    "    \n",
    "    arr = np.zeros(nc)\n",
    "    for i in xrange(nc):\n",
    "        if candidates[i] in model.vocab:\n",
    "            cand_vec = model[candidates[i]]\n",
    "#             arr[i] = (ref_vec * cand_vec).sum()\n",
    "            arr[i] = (ref_vec * (cand_vec / np.linalg.norm(cand_vec))).sum()\n",
    "#     l, r = arr[:, j].min(), arr[:, j].max()\n",
    "#     if r > l:\n",
    "#         arr[:, j] = (arr[:, j] - l) / (r - l)\n",
    "    return arr\n",
    "\n",
    "\n",
    "def cal_score(sentences, options):\n",
    "    n = len(sentences)\n",
    "    s = 0.\n",
    "    ref = 0.\n",
    "    for i in xrange(n):\n",
    "#         print sentences[i]\n",
    "#         print options[i][0], options[i][1]\n",
    "        b = cloze_test(sentences[i], options[i][0], options[i][1])\n",
    "        if b.argmax() == 0: s += 1.\n",
    "        if random.randint(0, 3) == 0: ref += 1.\n",
    "    return s/float(n), ref/float(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.578 , 0.2595\n",
      "0.5995 , 0.2415\n",
      "0.583 , 0.251\n",
      "0.5835 , 0.2505\n",
      "0.5775 , 0.243\n",
      "0.584 , 0.2565\n",
      "0.574 , 0.2585\n",
      "0.573 , 0.269\n",
      "0.595 , 0.2485\n",
      "0.562 , 0.2635\n"
     ]
    }
   ],
   "source": [
    "for i in xrange(10):\n",
    "    sentences, options = make_cloze(2000)\n",
    "    score, ref = cal_score(sentences, options)\n",
    "    print score, ',', ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('w2v_apple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec.load('w2v_apple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "print range(0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# s= u'有/在/附近/海域/作业/的/渔民/声称>>>v/p/f/n/n/uj/n/n'\n",
    "# idx = s.rfind('>>>')\n",
    "# t = s[idx + 3:]\n",
    "# print t\n",
    "\n",
    "# string = '越南空軍一架俄羅斯製造的蘇愷'\n",
    "# words = pseg.cut(string)\n",
    "# t = []\n",
    "# for w in words:\n",
    "#     t.append(w.word)\n",
    "# print '/'.join(t)\n",
    "#     print('%s %s' % (w.word, w.flag))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
